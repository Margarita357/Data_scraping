{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NASA Seeking Partner in Contest to Name Next Mars Rover'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and \n",
    "# collect the latest News Title and Paragraph Text. \n",
    "# Assign the text to variables that you can reference later.\n",
    "def news_title():\n",
    "    url1 = 'https://mars.nasa.gov/news/'\n",
    "    browser.visit(url1)\n",
    "    one_soup = bs(browser.html, 'lxml')\n",
    "\n",
    "\n",
    "    news_title = one_soup.find('div', class_=\"content_title\").text\n",
    "\n",
    "    return(news_title)\n",
    "\n",
    "news_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NASA has a class assignment for corporations, nonprofits and educational organizations involved in science and space exploration: partner with the agency to inspire future engineers and scientists by sponsoring a contest to name the next rover to venture to the Red Planet.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def news_article():\n",
    "    url1 = 'https://mars.nasa.gov/news/'\n",
    "    browser.visit(url1)\n",
    "    time.sleep(1)\n",
    "    one_soup = bs(browser.html, 'lxml')\n",
    "\n",
    "    news_p = one_soup.find('div', class_=\"article_teaser_body\").text\n",
    "\n",
    "    return(news_p)\n",
    "news_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars/spaceimages/images/mediumsize/PIA14884_ip.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visit the url for JPL Featured Space Image \n",
    "# [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "# Use splinter to navigate the site and find the image url for the current\n",
    "# Featured Mars Image and assign the url string to a variable called \n",
    "# `featured_image_url`.\n",
    "# Make sure to find the image url to the full size `.jpg` image.\n",
    "# Make sure to save a complete url string for this image.\n",
    "def featured_image(): \n",
    "    url2 = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(url2) \n",
    "    browser.click_link_by_id('full_image')\n",
    "    time.sleep(1)\n",
    "    two_soup = bs(browser.html, 'lxml')\n",
    "    \n",
    "    link = two_soup.find('img', class_ = \"fancybox-image\")\n",
    "    featured_image_url = url2 + link['src']\n",
    "    return(featured_image_url)\n",
    "featured_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1cf02b3ce075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfeatured_image_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatured_image_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfeatured_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-1cf02b3ce075>\u001b[0m in \u001b[0;36mfeatured_image\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"page-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfeatured_image_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatured_image_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfeatured_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "#attempt to get the full size image url\n",
    "\n",
    "# def featured_image(): \n",
    "#     url2 = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "#     browser.visit(url2) \n",
    "#     browser.click_link_by_id('full_image')\n",
    "#     time.sleep(1)\n",
    "#     browser.click_link_by_partial_href('/spaceimages/details.php?id=')\n",
    "#     time.sleep(1)\n",
    "#     two_soup = bs(browser.html, 'lxml')\n",
    "\n",
    "    \n",
    "#     link = two_soup.find('div', id = \"page-\")\n",
    "    \n",
    "#     featured_image_url = url2 + link\n",
    "#     return(featured_image_url)\n",
    "# featured_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jose Morales captured Mars from Chicago last night. 15000 frames for this Mars tonight.  The South Pole, Syrtis Major Planum, and Hellas Planitia are visible.pic.twitter.com/cFkgmdoHDV'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visit the Mars Weather twitter account [here]\n",
    "# (https://twitter.com/marswxreport?lang=en) and \n",
    "# scrape the latest Mars weather tweet from the page. \n",
    "# Save the tweet text for the weather report as a variable \n",
    "# called `mars_weather`.\n",
    "def weather():\n",
    "    url3 = 'https://twitter.com/marswxreport?lang=en'\n",
    "    browser.visit(url3) \n",
    "    three_soup = bs(browser.html, 'lxml')    \n",
    "\n",
    "    mars_weather = three_soup.find('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text\n",
    "    return(mars_weather)\n",
    "weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars Facts webpage [here](http://space-facts.com/mars/) \n",
    "# and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "def facts(): \n",
    "    url4 = 'http://space-facts.com/mars/'\n",
    "    # browser.visit(url4) \n",
    "    # four_soup = bs(browser.html, 'lxml')\n",
    "\n",
    "    tables = pd.read_html(url4)\n",
    "    df1 = tables[0]\n",
    "    df1.columns=[\"Category\", \"Measurement\"]\n",
    "    df1=df1.iloc[1:]\n",
    "    df1 = df1.set_index(\"Category\")\n",
    "    return(df1.to_html('table.html'))\n",
    "facts()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_html('table.html')\n",
    "#!open table.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Mars Hemispheres\n",
    "#Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) \n",
    "# to obtain high resolution images for each of Mar's hemispheres.\n",
    "#You will need to click each of the links to the hemispheres in order to \n",
    "# find the image url to the full resolution image.\n",
    "#Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. \n",
    "# Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "#Append the dictionary with the image url string and the hemisphere \n",
    "# title to a list. This list will contain one dictionary for each hemisphere.\n",
    "def hem_url_img():\n",
    "\n",
    "    url5 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    browser.visit(url5) \n",
    "\n",
    "    links = [(x.text, x['href']) for x in browser.find_by_css(\n",
    "        'div[class=\"description\"] a[class=\"itemLink product-item\"]'\n",
    "    )]\n",
    "\n",
    "    hemisphere_image_urls = []\n",
    "    for link in links:\n",
    "        browser.visit(link[1])\n",
    "        img_url = browser.find_by_css('img[class=\"wide-image\"]')['src']\n",
    "        hemisphere_image_urls.append({\n",
    "            'title': link[0].replace(' Hemisphere Enhanced', ''),\n",
    "            'img_url': img_url,\n",
    "        })\n",
    "    return(hemisphere_image_urls)\n",
    "hem_url_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6af4930c01b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-6af4930c01b3>\u001b[0m in \u001b[0;36mscrape\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnews_article2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfeatured_image2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatured_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mweather2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1cf02b3ce075>\u001b[0m in \u001b[0;36mfeatured_image\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"page-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfeatured_image_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatured_image_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfeatured_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "# Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "def scrape():\n",
    "    news_title2 = news_title()\n",
    "    \n",
    "    news_article2 = news_article()\n",
    "    \n",
    "    featured_image2 = featured_image()\n",
    "    \n",
    "    weather2 = weather()\n",
    "    \n",
    "    facts2 = facts()\n",
    "    \n",
    "    hem_url_img2 = hem_url_img()\n",
    "    \n",
    "    dict_ = {'news_title': news_title2, 'article': news_article2, 'image': featured_image2, 'weather': weather2, 'facts_table': facts2, \"hem\": hem_url_img2}\n",
    "    return(dict_)\n",
    "\n",
    "scrape()\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
